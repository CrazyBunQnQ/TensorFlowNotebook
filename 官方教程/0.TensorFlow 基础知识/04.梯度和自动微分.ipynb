{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFdPvlXBOdUN"
   },
   "source": [
    "# 梯度和自动微分简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6P32iYYV27b"
   },
   "source": [
    "## 自动微分和梯度\n",
    "\n",
    "[自动微分](https://en.wikipedia.org/wiki/Automatic_differentiation)对于实现机器学习算法（例如，用于训练神经网络的[反向传播](https://en.wikipedia.org/wiki/Backpropagation)）非常有用。\n",
    "\n",
    "在本指南中，您将探索使用 TensorFlow 计算梯度的方法，尤其是在 [Eager Execution](eager.ipynb) 中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MUXex9ctTuDB"
   },
   "source": [
    "## 设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "IqR2PQG4ZaZ0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHxb-dlhMIzW"
   },
   "source": [
    "## 计算梯度\n",
    "\n",
    "要实现自动微分，TensorFlow 需要记住在*前向传递*过程中哪些运算以何种顺序发生。随后，在*后向传递*期间，TensorFlow 以相反的顺序遍历此运算列表来计算梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CLWJl0QliB0"
   },
   "source": [
    "## 梯度带\n",
    "\n",
    "TensorFlow 为自动微分提供了 [`tf.GradientTape`](https://tensorflow.google.cn/api_docs/python/tf/GradientTape) API；即计算某个计算相对于某些输入（通常是 [`tf.Variable`](https://tensorflow.google.cn/api_docs/python/tf/Variable)）的梯度。TensorFlow 会将在 [`tf.GradientTape`](https://tensorflow.google.cn/api_docs/python/tf/GradientTape) 上下文内执行的相关运算“记录”到“磁带”上。然后，TensorFlow 使用该磁带来计算“记录”计算的梯度。该“记录”使用了[反向模式微分](https://en.wikipedia.org/wiki/Automatic_differentiation)。\n",
    "> 太绕了，可能翻译不准确，原文:\n",
    ">\n",
    "> TensorFlow provides the tf.GradientTape API for automatic differentiation; that is, computing the gradient of a computation with respect to some inputs, usually tf.Variables. TensorFlow \"records\" relevant operations executed inside the context of a tf.GradientTape onto a \"tape\". TensorFlow then uses that tape to compute the gradients of a \"recorded\" computation using reverse mode differentiation.\n",
    "\n",
    "例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Xq9GgTCP7a4A",
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  # x^2, x 的 2 次幂\n",
    "  y = x**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CR9tFAP_7cra"
   },
   "source": [
    "记录一些运算后，使用 [`GradientTape.gradient(target, sources)`](https://tensorflow.google.cn/api_docs/python/tf/GradientTape#gradient) 计算某个目标（通常是损失）相对于某个源（通常是模型变量）的梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LsvrwF6bHroC",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dy = 2x * dx\n",
    "dy_dx = tape.gradient(y, x)\n",
    "dy_dx.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2_aqsO25Vx1"
   },
   "source": [
    "上方示例使用标量，但是 [`tf.GradientTape`](https://tensorflow.google.cn/api_docs/python/tf/GradientTape) 在任何张量上都可以轻松运行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vacZ3-Ws5VdV",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'w:0' shape=(3, 2) dtype=float32, numpy=\n",
      "array([[ 0.02228432,  0.03483335],\n",
      "       [-0.8895191 , -0.51847446],\n",
      "       [-2.0239246 , -1.0687695 ]], dtype=float32)>\n",
      "<tf.Variable 'b:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>\n",
      "[[1.0, 2.0, 3.0]]\n"
     ]
    }
   ],
   "source": [
    "w = tf.Variable(tf.random.normal((3, 2)), name='w')\n",
    "print(w)\n",
    "b = tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')\n",
    "print(b)\n",
    "x = [[1., 2., 3.]]\n",
    "print(x)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "  y = x @ w + b\n",
    "  loss = tf.reduce_mean(y**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i4eXOkrQ-9Pb"
   },
   "source": [
    "要获得 `loss` 相对于两个变量的梯度，可以将这两个变量同时作为 `gradient` 方法的源传递。梯度带在关于源的传递方式上非常灵活，可以接受列表或字典的任何嵌套组合，并以相同的方式返回梯度结构（请参阅 [`tf.nest`](https://tensorflow.google.cn/api_docs/python/tf/nest)）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "luOtK1Da_BR0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[dl_dw, dl_db] = tape.gradient(loss, [w, b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ei4iVXi6qgM7"
   },
   "source": [
    "相对于每个源的梯度具有源的形状："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "aYbWRFPZqk4U",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n",
      "(3, 2)\n"
     ]
    }
   ],
   "source": [
    "print(w.shape)\n",
    "print(dl_dw.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dI_SzxHsvao1"
   },
   "source": [
    "此处也为梯度计算，这一次传递了一个变量字典："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "d73cY6NOuaMd",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([-7.828527, -4.208424], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_vars = {\n",
    "    'w': w,\n",
    "    'b': b\n",
    "}\n",
    "\n",
    "grad = tape.gradient(loss, my_vars)\n",
    "grad['b']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZ2LvHifEMgO"
   },
   "source": [
    "## 相对于模型的梯度\n",
    "\n",
    "通常将 `tf.Variables` 收集到 [`tf.Module`](https://tensorflow.google.cn/api_docs/python/tf/Module) 或其子类之一（[`layers.Layer`](https://tensorflow.google.cn/api_docs/python/tf/keras/layers/Layer)、[`keras.Model`](https://tensorflow.google.cn/api_docs/python/tf/keras/Model)）中，用于[设置检查点](checkpoint.ipynb)和[导出](saved_model.ipynb)。\n",
    "\n",
    "在大多数情况下，需要计算相对于模型的可训练变量的梯度。 由于 `tf.Module` 的所有子类都在 [`Module.trainable_variables`](https://tensorflow.google.cn/api_docs/python/tf/Module#trainable_variables) 属性中聚合其变量，您可以用几行代码计算这些梯度： "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "JvesHtbQESc-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer = tf.keras.layers.Dense(2, activation='relu')\n",
    "x = tf.constant([[1., 2., 3.]])\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  # 前向传递(Forward pass)\n",
    "  y = layer(x)\n",
    "  loss = tf.reduce_mean(y**2)\n",
    "\n",
    "# 计算每个可训练变量的梯度\n",
    "grad = tape.gradient(loss, layer.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "PR_ezr6UFrpI",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense/kernel:0, shape: (3, 2)\n",
      "dense/bias:0, shape: (2,)\n"
     ]
    }
   ],
   "source": [
    "for var, g in zip(layer.trainable_variables, grad):\n",
    "  print(f'{var.name}, shape: {g.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6Gx6LS714zR"
   },
   "source": [
    "<a id=\"watches\"></a>\n",
    "\n",
    "## 控制梯度带监视的内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N4VlqKFzzGaC"
   },
   "source": [
    "默认行为是在访问可训练 `tf.Variable` 后记录所有运算。原因如下：\n",
    "\n",
    "- 梯度带需要知道在前向传递中记录哪些运算，以计算 后向传递 中的梯度。\n",
    "- 梯度带包含对中间输出的引用，因此应避免记录不必要的操作。\n",
    "- 最常见用例涉及计算损失相对于模型的所有可训练变量的梯度。\n",
    "\n",
    "以下示例无法计算梯度，因为默认情况下 [`tf.Tensor`](https://tensorflow.google.cn/api_docs/python/tf/Tensor) 未被“监视”，并且 `tf.Variable` 不可训练："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Kj9gPckdB37a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 可训练变量\n",
    "x0 = tf.Variable(3.0, name='x0')\n",
    "# 不可训练\n",
    "x1 = tf.Variable(3.0, name='x1', trainable=False)\n",
    "# 不是变量：变量+张量返回一个张量。\n",
    "x2 = tf.Variable(2.0, name='x2') + 1.0\n",
    "# 不是变量\n",
    "x3 = tf.constant(3.0, name='x3')\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  y = (x0**2) + (x1**2) + (x2**2)\n",
    "\n",
    "grad = tape.gradient(y, [x0, x1, x2, x3])\n",
    "\n",
    "for g in grad:\n",
    "  print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkcpQnLgNxgi"
   },
   "source": [
    "您可以使用 `GradientTape.watched_variables` 方法列出梯度带正在监视的变量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "hwNwjW1eAkib",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x0:0']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[var.name for var in tape.watched_variables()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NB9I1uFvB4tf"
   },
   "source": [
    "`tf.GradientTape` 提供了钩子，让用户可以控制被监视或不被监视的内容。\n",
    "\n",
    "要记录相对于 `tf.Tensor` 的梯度，您需要调用 [`GradientTape.watch(x)`](https://www.tensorflow.org/api_docs/python/tf/GradientTape#watch)："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "tVN1QqFRDHBK",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "  tape.watch(x)\n",
    "  y = x**2\n",
    "\n",
    "# dy = 2x * dx\n",
    "dy_dx = tape.gradient(y, x)\n",
    "print(dy_dx.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qxsiYnf2DN8K"
   },
   "source": [
    "相反，要停用监视所有 `tf.Variables` 的默认行为，请在创建梯度带时设置 `watch_accessed_variables=False`。此计算使用两个变量，但仅连接其中一个变量的梯度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "7QPzwWvSEwIp",
    "tags": []
   },
   "outputs": [],
   "source": [
    "x0 = tf.Variable(0.0)\n",
    "x1 = tf.Variable(10.0)\n",
    "\n",
    "with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "  tape.watch(x1)\n",
    "  y0 = tf.math.sin(x0)\n",
    "  y1 = tf.nn.softplus(x1)\n",
    "  y = y0 + y1\n",
    "  ys = tf.reduce_sum(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRduLbE1H2IJ"
   },
   "source": [
    "- [`tf.nn.softplus(x1)`](https://www.tensorflow.org/api_docs/python/tf/math/softplus): 计算 elementwise softplus. 等价于 `softplus(x) = log(exp(x) + 1)`\n",
    "\n",
    "由于 `GradientTape.watch` 未在 `x0` 上调用，未相对于它计算梯度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "e6GM-3evH1Sz",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy/dx0: None\n",
      "dy/dx1: 0.9999546\n"
     ]
    }
   ],
   "source": [
    "# dys/dx1 = exp(x1) / (1 + exp(x1)) = sigmoid(x1)\n",
    "grad = tape.gradient(ys, {'x0': x0, 'x1': x1})\n",
    "\n",
    "print('dy/dx0:', grad['x0'])\n",
    "print('dy/dx1:', grad['x1'].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2g1nKB6P-OnA"
   },
   "source": [
    "- [`sigmoid(x1)`](https://www.tensorflow.org/api_docs/python/tf/math/sigmoid): 按元素计算 x 的 sigmoid\n",
    "    - 公式: $\\mathrm{sigmoid}(x) = y = 1 / (1 + \\exp(-x))$\n",
    "    - $x \\in (-\\infty, \\infty)$, $\\mathrm{sigmoid}(x) \\in (0, 1)$\n",
    "\n",
    "## 中间结果\n",
    "\n",
    "您还可以请求输出相对于 [`tf.GradientTape`](https://www.tensorflow.org/api_docs/python/tf/GradientTape) 上下文中计算的中间值的梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "7XaPRAwUyYms",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  tape.watch(x)\n",
    "  y = x * x\n",
    "  z = y * y\n",
    "\n",
    "# 使用磁带计算 z 相对于中间值 y 的梯度(Use the tape to compute the gradient of z with respect to the intermediate value y.)\n",
    "# dz_dy = 2 * y and y = x ** 2 = 9\n",
    "print(tape.gradient(z, y).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISkXuY7YzIcS"
   },
   "source": [
    "默认情况下，只要调用 `GradientTape.gradient` 方法，就会释放 `GradientTape` 保存的资源。要在同一计算中计算多个梯度，请创建一个 `persistent=True` 的梯度带。这样一来，当梯度带对象作为垃圾回收时，随着资源的释放，可以对 `gradient` 方法进行多次调用。例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "zZaCm3-9zVCi",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4. 108.]\n",
      "[2. 6.]\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([1, 3.0])\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "  tape.watch(x)\n",
    "  y = x * x\n",
    "  z = y * y\n",
    "\n",
    "print(tape.gradient(z, x).numpy())  # 108.0 (4 * x**3 at x = 3)\n",
    "print(tape.gradient(y, x).numpy())  # 6.0 (2 * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "j8bv_jQFg6CN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "del tape   # 删除对磁带的引用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_ZY-9BUB7vX"
   },
   "source": [
    "## 性能说明\n",
    "\n",
    "- 在梯度带上下文内进行运算会有一个微小的开销。对于大多数 Eager Execution 来说，这一成本并不明显，但是您仍然应当仅在需要的地方使用梯度带上下文。\n",
    "\n",
    "- 梯度带使用内存来存储中间结果，包括输入和输出，以便在后向传递中使用。\n",
    "\n",
    "    为了提高效率，某些运算（例如 `ReLU`）不需要保留中间结果，而是在前向传递中进行剪枝。不过，如果在梯度带上使用 `persistent=True`，则*不会丢弃任何内容*，并且峰值内存使用量会更高。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9dLBpZsJebFq"
   },
   "source": [
    "## 非标量目标的梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7pldU9F5duP2"
   },
   "source": [
    "梯度从根本上说是对标量的运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "qI0sDV_WeXBb",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "-0.25\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(2.0)\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "  y0 = x**2\n",
    "  y1 = 1 / x\n",
    "\n",
    "print(tape.gradient(y0, x).numpy())\n",
    "print(tape.gradient(y1, x).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COEyYp34fxj4"
   },
   "source": [
    "因此，如果需要多个目标的梯度，则每个源的结果为：\n",
    "\n",
    "- 目标总和的梯度，或等效\n",
    "- 每个目标的梯度总和。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "o4a6_YOcfWKS",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.75\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(2.0)\n",
    "with tf.GradientTape() as tape:\n",
    "  y0 = x**2\n",
    "  y1 = 1 / x\n",
    "\n",
    "print(tape.gradient({'y0': y0, 'y1': y1}, x).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvP-mkBMgbym"
   },
   "source": [
    "类似地，如果目标不是标量，则计算总和的梯度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "DArPWqsSh5un",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(2.)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  y = x * [3., 4.]\n",
    "\n",
    "print(tape.gradient(y, x).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "flDbx68Zh5Lb"
   },
   "source": [
    "这样一来，就可以轻松获取损失集合总和的梯度，或者逐元素损失计算总和的梯度。\n",
    "\n",
    "如果每个条目都需要单独的梯度，请参阅[Jacobians](https://www.tensorflow.org/guide/advanced_autodiff#jacobians)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwFswok8RAly"
   },
   "source": [
    "在某些情况下，您可以跳过 Jacobians。对于逐元素计算，总和的梯度给出了每个元素相对于其输入元素的导数，因为每个元素都是独立的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "JQvk_jnMmTDS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = tf.linspace(-10.0, 10.0, 200+1)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  tape.watch(x)\n",
    "  y = tf.nn.sigmoid(x)\n",
    "\n",
    "dy_dx = tape.gradient(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "e_f2QgDPmcPE",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, y, label='y')\n",
    "plt.plot(x, dy_dx, label='dy/dx')\n",
    "plt.legend()\n",
    "_ = plt.xlabel('x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6kADybtQzYj4"
   },
   "source": [
    "- [`tf.linspace(-10.0, 10.0, 200+1)`](https://www.tensorflow.org/api_docs/python/tf/linspace): 沿给定轴在间隔内生成均匀间隔的值。\n",
    "\n",
    "## 控制流\n",
    "\n",
    "在执行运算时，由于梯度带会记录这些运算，因此会自然地处理 Python 控制流（例如 `if` 和 `while` 语句）。\n",
    "\n",
    "此处，`if` 的每个分支上使用不同变量。梯度仅连接到使用的变量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "ciFLizhrrjy7",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(1.0)\n",
    "\n",
    "v0 = tf.Variable(2.0)\n",
    "v1 = tf.Variable(2.0)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "  tape.watch(x)\n",
    "  if x > 0.0:\n",
    "    result = v0\n",
    "  else:\n",
    "    result = v1**2 \n",
    "\n",
    "dv0, dv1 = tape.gradient(result, [v0, v1])\n",
    "\n",
    "print(dv0)\n",
    "print(dv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKnLaiapsjeP"
   },
   "source": [
    "注意，控制语句本身不可微分，因此对基于梯度的优化器不可见。\n",
    "\n",
    "根据上面示例中 `x` 的值，梯度带将记录 `result = v0` 或 `result = v1**2`。 相对于 `x` 的梯度始终为 `None`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "8k05WmuAwPm7",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "dx = tape.gradient(result, x)\n",
    "\n",
    "print(dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egypBxISAHhx"
   },
   "source": [
    "## 获取 `None` 的梯度\n",
    "\n",
    "当目标未连接到源时，您将获得 `None` 的梯度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "CU185WDM81Ut",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(2.)\n",
    "y = tf.Variable(3.)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  z = y * y\n",
    "print(tape.gradient(z, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZbKpHfBRJym"
   },
   "source": [
    "此处 `z` 显然未连接到 `x`，但可以通过几种不太明显的方式将梯度断开。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHDzDOiQ8xmw"
   },
   "source": [
    "### 1. 使用张量替换变量\n",
    "\n",
    "在[控制梯度带监视内容](#watches)部分中，梯度带会自动监视 `tf.Variable`，但不会监视 `tf.Tensor`。\n",
    "\n",
    "一个常见错误是无意中将 `tf.Variable` 替换为 `tf.Tensor`，而不使用 `Variable.assign` 更新 `tf.Variable`。见下例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "QPKY4Tn9zX7_",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResourceVariable : tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "EagerTensor : None\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(2.0)\n",
    "\n",
    "for epoch in range(2):\n",
    "  with tf.GradientTape() as tape:\n",
    "    y = x+1\n",
    "\n",
    "  print(type(x).__name__, \":\", tape.gradient(y, x))\n",
    "  x = x + 1   # 这应该是 `x.assign_add(1)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3gwZKxgA97an"
   },
   "source": [
    "### 2.在 TensorFlow 之外进行了计算\n",
    "\n",
    "如果计算退出 TensorFlow，梯度带将无法记录梯度路径。例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "jmoLCDJb_yw1",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable([[1.0, 2.0],\n",
    "                 [3.0, 4.0]], dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  x2 = x**2\n",
    "\n",
    "  # 这一步是用 NumPy 计算的\n",
    "  y = np.mean(x2, axis=0)\n",
    "\n",
    "  # 像大多数操作一样，reduce_mean 将使用 `tf.convert_to_tensor` 把 NumPy 数组转换为常量张量\n",
    "  y = tf.reduce_mean(y, axis=0)\n",
    "\n",
    "print(tape.gradient(y, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3YVfP3R-tp7"
   },
   "source": [
    "### 3.通过整数或字符串获取梯度\n",
    "\n",
    "整数和字符串不可微分。如果计算路径使用这些数据类型，则不会出现梯度。\n",
    "\n",
    "谁也不会期望字符串是可微分的，但是如果不指定 `dtype`，很容易意外创建一个 `int` 常量或变量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "9jlHXHqfASU3",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\n",
      "WARNING:tensorflow:The dtype of the target tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\n",
      "WARNING:tensorflow:The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(10)\n",
    "\n",
    "with tf.GradientTape() as g:\n",
    "  g.watch(x)\n",
    "  y = x * x\n",
    "\n",
    "print(g.gradient(y, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RsdP_mTHX9L1"
   },
   "source": [
    "TensorFlow 不会在类型之间自动进行转换，因此，在实践中，您经常会遇到类型错误而不是缺少梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WyAZ7C8qCEs6"
   },
   "source": [
    "### 4. 通过有状态对象获取梯度\n",
    "\n",
    "状态会停止梯度。从有状态对象读取时，梯度带只能观察当前状态，而不能观察导致该状态的历史记录。\n",
    "\n",
    "`tf.Tensor` 不可变。张量创建后就不能更改。它有一个*值*，但没有*状态*。目前讨论的所有运算也都无状态：[`tf.matmul`](https://www.tensorflow.org/api_docs/python/tf/linalg/matmul) 的输出只取决于它的输入。\n",
    "\n",
    "`tf.Variable` 具有内部状态，即它的值。使用变量时，会读取状态。计算相对于变量的梯度是正常操作，但是变量的状态会阻止梯度计算进一步向后移动。 例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "C1tLeeRFE479",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "x0 = tf.Variable(3.0)\n",
    "x1 = tf.Variable(0.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  # 更新 x1 = x1 + x0.\n",
    "  x1.assign_add(x0)\n",
    "  # 磁带从 x1 开始录制\n",
    "  y = x1**2   # y = (x1 + x0)**2\n",
    "\n",
    "# 这里行不通\n",
    "print(tape.gradient(y, x0))   #dy/dx0 = 2*(x1 + x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKA92-dqF2r-"
   },
   "source": [
    "类似地，[`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) 迭代器和 [`tf.queue`](https://www.tensorflow.org/api_docs/python/tf/queue) 也有状态，会停止经过它们的张量上的所有梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HHvcDGIbOj2I"
   },
   "source": [
    "## 未注册梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aoc-A6AxVqry"
   },
   "source": [
    "某些 [`tf.Operation`](https://www.tensorflow.org/api_docs/python/tf/Operation) 被**注册为不可微分**，将返回 `None`。还有一些则**未注册梯度**。\n",
    "\n",
    "[`tf.raw_ops`](https://www.tensorflow.org/api_docs/python/tf/raw_ops) 页面显示了哪些低级运算已经注册梯度。\n",
    "\n",
    "如果您试图通过一个没有注册梯度的浮点运算获取梯度，梯度带将抛出错误，而不是直接返回 `None`。这样一来，您可以了解某个环节出现问题。\n",
    "\n",
    "例如，[`tf.image.adjust_contrast`](https://www.tensorflow.org/api_docs/python/tf/image/adjust_contrast) 函数封装了 [`raw_ops.AdjustContrastv2`](https://www.tensorflow.org/api_docs/python/tf/raw_ops/AdjustContrastv2)，此运算可能具有梯度，但未实现该梯度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "HSb20FXc_V0U",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LookupError: gradient registry has no entry for: AdjustContrastv2\n"
     ]
    }
   ],
   "source": [
    "image = tf.Variable([[[0.5, 0.0, 0.0]]])\n",
    "delta = tf.Variable(0.1)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  new_image = tf.image.adjust_contrast(image, delta)\n",
    "\n",
    "try:\n",
    "  print(tape.gradient(new_image, [image, delta]))\n",
    "  assert False   # 这里不应该发生\n",
    "except LookupError as e:\n",
    "  print(f'{type(e).__name__}: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDoutjzATiEm"
   },
   "source": [
    "如果需要通过此运算进行微分，则需要实现梯度并注册该梯度（使用 [`tf.RegisterGradient`](https://www.tensorflow.org/api_docs/python/tf/RegisterGradient)），或者使用其他运算重新实现该函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCTwc_dQXp2W"
   },
   "source": [
    "## 零而不是 None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYDrVogA89eA"
   },
   "source": [
    "在某些情况下，对于未连接的梯度，得到 0 而不是 `None` 会比较方便。您可以使用 `unconnected_gradients` 参数来决定具有未连接的梯度时返回的内容："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "U6zxk1sf9Ixx",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0. 0.], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable([2., 2.])\n",
    "y = tf.Variable(3.)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  z = y**2\n",
    "print(tape.gradient(z, x, unconnected_gradients=tf.UnconnectedGradients.ZERO))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Tce3stUlHN0L"
   ],
   "name": "autodiff.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
